The plan: repro as much as possible of Karpathy's blog post The Unreasonable Effectiveness of Recurrent Neural Networks

2025-09-03

Found article: https://karpathy.github.io/2015/05/21/rnn-effectiveness/

Read through, there's a lot there!

# Recurrent Neural Networks

Diffs between (VanillaNNs and CNNs) and RNNs: fixed-size input vector.  Interestingly, LLMs have a fixed-size input -- the context length.
Likewise fixed number of computational steps.  This is touted as an advantage, though of course there's no real value in
spending as long completing "the fat cat sat on the mat" as you do completing a 100k token chat transcript.

Do real-world LLMs trim the sequence length to match the actual size?  Using padding tokens feels like a waste.  But then
batches are hard as you'd want them all to be the same size.

But of course for RNNs the hidden state is fixed-size so you have the bottleneck.  It doesn't matter how long you spend
thinking about stuff if you're forgetting things all of the time.

RNNs being turing-complete presumably assumes infinite precision in the hidden state.

Sequences for not-obviously sequential stuff (eg. gaze pattern) is interesting, I remember reading about that
at the time.

His explanation of RNNs is interesting and while compatible with my own, differs
in some interesting ways.  My "hidden states are hidden outputs" is a useful thing for understanding how things are
unrolled, but the practice is of course different.  Perhaps that doesn't matter in actual training?

tanh for activation function interesting, and I note it's only applied to the hidden state update (and the Y is generated
purely on the hidden state after the update, which makes sense otherwise you'd need to do it on X and then H).

Worth noting: to do multi-layer, he just defines a class then calls it twice.  Makes sense!  You can just do that and
then run backwards with PyTorch, I think.  Will be worth experimenting.

LSTMs are used in the essay, and they're more complex.  Worth noting.  Will want to dig into that, I think.


# Character-Level Language Models

OK, so we're not tokenising.  That's important and part of what makes this interesting to me.

Explanation is clear, but the error function is not specified and we'll need to think about that.

Note softmax is cropping up again!

"The RNN is trained with mini-batch Stochastic Gradient Descent and I like to use RMSProp or Adam (per-parameter adaptive learning rate methods) to stablilize the updates."

Code at https://gist.github.com/karpathy/d4dee566867f8291f086 looks useful to learn from.  However, it's RNN rather than LSTM.

https://github.com/karpathy/char-rnn perhaps more relevant?  It links to https://cs.stanford.edu/people/karpathy/char-rnn/, which has the shakespeare, war and peace and wikipedia datasets.  Not the PG one, of course -- copyright would be a problem there.  And the Linux one is just the kernel, not the full dataset he trained on.

# Fun with RNNs

## Paul Graham generator

"this can be cut in half with 50 character BPTT at negligible cost in performance" -- asked ChatGPT and after some discussion it tells me that my gloss on what it said as "with 50-character BPTT, we run a 100-character sequence through but only backprop through the last 50 characters-worth" is a good summary.

"We can also play with the temperature of the Softmax during sampling".  Interesting.  I'd thought of temperature as being
something used when selecting from output options provided by the probability distribution provided by softmax, not an input into
softmax itself.  Chatting with ChatGPT, I came to a better understanding.  My summary (confirmed by it):

We never use argmax -- instead, we choose an output value by dividing our logits by the temperature, then running it through softmax. That gives us a probability distribution, and we always pick from that based on probabilities, so regardless of temperature, if softmax gives us (0.1, 0.4, 0.5) we always pick the first one 10% of the time, the second 40%, and the last 50%.

So, Karpathy is not being technically accurate (though he is phrasing things exactly as people in the industry do).  Temperature
is not part of softmax itself, but nor is it (like I'd imagined) a parameter used when choosing a next character based on the
post-softmax probabilities.

Instead, we just divide the logits by it before running them through softmax.  This has the effect of flattening the distribution
if T > 1, or sharpening it if T < 1.  That also explains why he says "setting temperature very near zero will give the most likely thing that Paul Graham might say" -- if we set it to exactly zero we'd get a divide by zero.

Might be worth a post or at least a callout somewhere later in the LLM from scratch series or in this one.  With a worked example showing
how the distribution changes.


Other examples interesting but not much to learn there.


# Understanding what’s going on

All seems reasonably clear, but there are some nice visualisations.  I particularly like the ones showing the text with the colour
whether the neuron under examination is firing.


# Source code

I think that it would be good to build my own, with LLM help, but his source will be a useful reference as needed.

Interesting reference that Torch was cool then -- but of course PyTorch won out.

# Further Reading

Note lots of references to Sutskever

"Currently it seems that word-level models work better than character-level models, but this is surely a temporary thing" -- hmm

"A second issue is they unnecessarily couple their representation size to the amount of computation per step. For instance, if you double the size of the hidden state vector you’d quadruple the amount of FLOPS at each step due to the matrix multiplication. Ideally, we’d like to maintain a huge representation/memory (e.g. containing all of Wikipedia or many intermediate state variables), while maintaining the ability to keep computation per time step fixed."

Hinting at fixed-length bottleneck here.

"The concept of attention is the most interesting recent architectural innovation in neural networks."

Interesting that attention gets mentioned.  Same meaning as now?  (Obviously not identical, but...)

"unfortunately one sacrifices efficiency because everything that can be attended to is attended to" well yes.

"This then calls for use of techniques from the Reinforcement Learning literature (e.g. REINFORCE) where people are perfectly used to the concept of non-differentiable interactions"

Hmmm.  So RL does not depend on differentiability.  That pushes it high up my list of things to learn.

The "People" bit at the end has what look like interesting links (though they go to individuals' homepages rather than the papers in question)










----ends


Chat with ChatGPT: https://chatgpt.com/c/68b87061-0b58-8326-98d6-1be84411d10e

Some interesting stuff about RNNs (some of which is mixed in above).

I was originally thinking of a post titled "reproducing karpathy" but thinking about it, maybe more of a "revisiting Karpathy".
Or something more specific, it would be silly to imply that this is the most important (or even one of the top ten most important)
things he's done.

Proposed path:

1. intro the article, brief summary, perhaps some high-level gloss on the above.
2. implementation: use PyTorch's built-in LSTM, with parameters chosen from the article/accompanying source.  Train and generate on K's
test data, see if we get similar results.  I think this will need to have the temperature calculations I discussed with ChatGPT.
3. building an LSTM, step 1: build an RNN from "scratch" (in PyTorch, of course, so not reall from scratch).  Try to train, show vanishing gradients problem
4. Step 2, make it an LSTM.  Show it works.


Series title, brainstormed with ChatGPT:

Old-school language models
Vintage language models
Retro language models

This as a start for each post's title, with the following per-post titles:

Revisiting Karpathy’s “The Unreasonable Effectiveness of Recurrent Neural Networks”
Reproducing results with PyTorch’s LSTM
Rolling our own RNN cell
From RNN to LSTM

I'm sure of the first, the next three could do with some tightening




2025-09-04

Let's get started with the Shakespeare example, given that the Paul Graham dataset isn't available (for good reason).

First step, a dummy script to make sure that I understand the imports and so on (I've set up a uv environment with
ipython, numpy and PyTorch):

```
giles@perry:~/Dev/karpathy-repro (main)$ uv run shakespeare.py
giles@perry:~/Dev/karpathy-repro (main)$ cat shakespeare.py
import torch


lstm = torch.nn.LSTM(input_size=2, hidden_size=512, num_layers=3)
giles@perry:~/Dev/karpathy-repro (main)$ cat pyproject.toml
[project]
name = "karpathy-repro"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "ipython>=9.5.0",
    "numpy>=2.3.2",
    "torch>=2.8.0",
]
```


OK, we have working imports -- and the hidden size and num_layers match the blog post.

Now, let's work out what our input size is.  We're doing characters in one-hot vectors, so it will be
something to do with the character vocab.  Simplest possible thing would be to assume ASCII and have 128 inputs, but let's see the source
at https://github.com/karpathy/char-rnn

Note on the code that I spotted almost right away: I think he's doing the backward pass manually!  See lines 227-293 in train.lua

OK, at lines 111-112 we have:

local loader = CharSplitLMMinibatchLoader.create(opt.data_dir, opt.batch_size, opt.seq_length, split_sizes)
local vocab_size = loader.vocab_size  -- the number of distinct characters

...and at lines 147 onwards we create the model (it supports LSTM, GRU -- whatever that is -- and RNN) passing in vocab_size as the first parameter.

So, we're dynamically working out the vocab size from the input.  Shiny!

CharSplitLMMinibatchLoader is a Lua Torch thing.  Let's see what PyTorch has to offer.

https://docs.pytorch.org/docs/stable/data.html looks like a good starting point.  Now, DataLoader has as its first parameter a Dataset object.
That take us to https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset.  Not much help there, though.

Let's work out what his code is actually doing.  CharSplitLMMinibatchLoader is a class that he defines, in https://github.com/karpathy/char-rnn/blob/master/util/CharSplitLMMinibatchLoader.lua

It looks in the given directory for a file called input.txt, and also vocab.t7 and data.t7.  These are the input text, a vocab file and
a tensor file.  If they don't exist, it creates them with CharSplitLMMinibatchLoader.text_to_tensor.

That function loads up the input file, iterates over all of the characters and puts them into a table (~=dict) called unordered where a character
maps to true if it exists.  It also keeps track of total characters read (as in, this will wind up being the length of the file) in tot_len.

It then iterates over that table, and puts it into another table mapping from a number 1..N to the actual character, which is a lua array.
It then sorts that, so we have what amounts to a sorted array of all characters (1-indexed, this being lua).

Finally, it swaps it around so that it's mapping character to int ID, calling that vocab_mapping.  That is what will be saved to the vocab file (this happens at the end of the
function).  It uses torch.save for that, but it's just a lua table, not anything torch-specific.

It then creates a tensor version of the input data.  It starts by creating a torch.ByteTensor (so now we're getting some useful info about how
it handles charsets -- it doesn't!  it's actually a byte-level RNN).  The length is tot_len, which is the number of bytes in the file.  Then it opens
the file again and iterates over the characters, and uses the vocab_mapping table to work out which int ID they have, and pops them into the tensor.

Finally, it saves the vocab mapping (as I said before) and the tensor file.

Well, yuk.  I don't know how much of that mess is due to lua being lua (you have to read into a buffer and manage short reads like you would in
C, for example), and how much is academics-can't-code, but I'm sure it would be easier in Python.  Let's give that a go.


OK, here's some code.

giles@perry:~/Dev/karpathy-repro (main)$ cat load_data_test.py
import time

import click
import torch


@click.command()
@click.argument("filename")
def main(filename):
    start = time.time()
    with open(filename, "rb") as f:
        data = f.read()

    characters = set(data)
    byte_to_id = {byte: ii for (ii, byte) in enumerate(sorted(characters))}
    data = torch.tensor([byte_to_id[byte] for byte in data], dtype=torch.long)
    print(time.time() - start)



if __name__ == "__main__":
    main()

Important stuff:

* We want our vocab to be the set of characters in the input.  IMO given that this is all bytewise, it would be easier
  to just have a vocab of all bytes and a size of 256.
* The IDs should be 0..N-1 where N is the vocab size, and bytes should reflect the lexicographic sort order -- eg. for the vocab ["a", "d", z"],
  they should be ordered 0, 1, 2 in that order.  Importantly, IDs should not follow time to first encounter in the data.

Apart from the 0-indexing of the token IDs to match Python standard rather than Lua's 1-indexing, this is essentially the same.  And
I don't think it's worth stashing the output in files:

giles@perry:~/Dev/karpathy-repro (main)$ uv run load_data_test.py shakespeare.txt
0.049793243408203125

0.05s to load in the whole 1.1M Shakespeare dataset.  If we were building a general tool like AK was, then it would be worthwhile, but
we're not.

NOTE: he says that the dataset he used was 4.4MiB.  Presumably a different one, sigh


Right, once we have that data loaded, we need to put it into some kind of DataLoader for training.  ChatGPT tells me that there's no standard
way to do this, but there is apparently a PyTorch word_language_model example which is similar.  Let's take a look.

https://github.com/pytorch/examples/tree/main/word_language_model

"This example trains a multi-layer RNN (Elman, GRU, or LSTM) or Transformer on a language modeling task"

Sounds good.

However, it looks a little low-level.  Need something more basic.  This looks solid, and very Karpathy-like: https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial?utm_source=chatgpt.com

OK, here's what we need to think about.  The code there reads in a whole bunch of names, so that we can learn how to generate them.
Each name is a self-contained dataset.

What we're trying to do here is much more like training an LLM.  We don't have a set of examples per se; we have one text -- in this first case, the
one I'm working with, it's 1.1MiB of Shakespeare.

That needs to somehow be split into a series of training examples.  I'm thinking we need to break it up into sequences of a given length.

K's loader takes parameters data_dir, batch_size, seq_length, split_fractions.  After the file-wrangling, we have a thing called "data" which is
a 1-d tensor of character IDs -- that is, one per byte in the original text.  And we have vocab_mapping, which is the mapping from chars to IDs.  (I assume
he generates the inverse mapping later).

Next, we have this:

    local len = data:size(1)
    if len % (batch_size * seq_length) ~= 0 then
        print('cutting off end of data so that the batches/sequences divide evenly')
        data = data:sub(1, batch_size * seq_length
                    * math.floor(len / (batch_size * seq_length)))
    end


So: we have a standardised sequence length, and a batch size.  We chop off all data to ignore anything after batch_size * seq_length.
Seems wasteful -- I'm sure we could do better with some kind of incomplete batch, perhaps overlapping sequences, maybe padded ones at the end.
But perhaps that doesn't matter.

Had a chat with ChatGPT about this.  Here's the explanation I came up with, which it approved of:

----
We have a file of (say) 50,001 characters.

We split into 1,000 chunks of Xs, each 50 characters long, and 1,000 chunks of Ys, each of which is 50 characters long (and is the shifted-right letters that correspond to the Xs).

We run the sample X_1 through the RNN, and then calculate loss against Y_1.

We then backprop, which naturally goes through 50 layers'-worth of unrolling.

We then clear out our gradients, but we do *not* clear out the hidden state.  It is, however, "detached" in some way so that gradients don't keep flowing back.

Now we run X_2 through the RNN, and calculate loss against Y_2. Backprop again, 50 layers again. Rinse and repeat.
----

In my mind, this means that the RNN is learning how to deal with a complex hidden state with all kinds of stuff in it,
and it's learning how to generate that state, but it only learns how to update the state based on the sequence length,
and only learns how 50 characters of stuff might be affected by it.

Need to think about that a bit.

Anyway: what we need is that dataset.  What I think we need from a PyTorch perspective is to split it into sequence_length
sized chunks and work out the Xs and Ys.

I was a bit confused by the NamesDataset at https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial?utm_source=chatgpt.com,
because I (rightly) thought that "data" meant Xs and "labels" meant Ys, but they weren't doing the shift-right stuff for the Ys.
ChatGPT put me straight -- it's a classifier, so the Ys are the nationality of the names that are in the Xs.

So: So what we'll need to do in our example is split up the input text, convert to one-hots, then build a dataset like that with the data_tensors set to the input one-hots and label_tensors set to the shifted-right ones.

data and labels are human-friendly representations of the data items, so we can set them to the text in question.  I think
we're in a good position to get started.

After some back and forth, we have this:

```
import os
import time

import click
import torch

from torch.utils.data import Dataset


class NextByteDataset(Dataset):

    def __init__(self, full_data, seq_length):
        super().__init__()

        assert seq_length > 0, "Sequence length must be > 0"
        self.seq_length = seq_length

        self.num_sequences = (len(full_data) - 1) // self.seq_length
        assert self.num_sequences > 0, "Not enough data for any sequences"

        self.data = full_data[:(self.num_sequences * self.seq_length) + 1]

        self.vocab = sorted(set(self.data))
        self.id_to_byte = {ii: byte for (ii, byte) in enumerate(self.vocab)}
        self.byte_to_id = {
            byte: ii
            for (ii, byte) in self.id_to_byte.items()
        }
        self.data_as_ids = torch.tensor(
            [self.byte_to_id[b] for b in self.data],
            dtype=torch.long
        )


    def __len__(self):
        return self.num_sequences


    def __getitem__(self, ix):
        start = ix * self.seq_length
        end = start + self.seq_length

        xs = self.data[start:end]
        x_ids = self.data_as_ids[start:end]

        ys = self.data[start + 1:end + 1]
        y_ids = self.data_as_ids[start + 1:end + 1]


        return x_ids, y_ids, xs, ys



def load_dataset(directory, seq_length):
    filename = os.path.join(directory, "input.txt")
    with open(filename, "rb") as f:
        data = f.read()

    return NextByteDataset(data, seq_length)


@click.command()
@click.argument("directory")
@click.argument("seq_length", type=int)
def main(directory, seq_length):
    start = time.time()
    dataset = load_dataset(directory, seq_length)
    print(time.time() - start)

    print(len(dataset))
    print(dataset[0])


if __name__ == "__main__":
    main()
```

Disregarding the test harness we have:

* We're not converting to one-hots in the dataset like the tutorial did.  We could, but ChatGPT suggests that we do that as-needed,
  which doesn't sound crazy.
* The ordering of the return values differs from the tutorial.  It does Y_for_nn, X_for_nn, Y_raw, X_raw, where the raw values are largely for
  debugging.  We're swapping around X and Y.  This is apparently because classification tasks like the tutorial tend to have the label first, while with language
  processing people do X first.  TBH I prefer it.

So, we have a simple dataloader that loads up the file, splits it so that it has a round number of sequences-worth (note the -1 in num_sequences
because we need to allow for the fact that Y is shifted right a character, so we need one extra for that, and the matching +1 in the full_data slice),
gets the vocab by finding all unique bytes then sorting it so that we match K's lexicographic IDs for bytes, generate an id to byte mapping (which
will be needed for decoding -- YAGNI doesn't apply because WAGNI), flip that to get our byte_to_id dict, then create a torch.tensor with the data
converted into IDs.  Our length is the number of sequences we have, and __getitem__ just returns slices into the ID-ified tensor for the values
we're going to be running through/receiving from the RNN (as it's a tensor this will probably be views, so no copying required), and then
relatively cheap slices of the byte array for the raw stuff, which I believe is only ever used for debugging anyway.

Quite a lot of work want into that with a bunch of circling around, but I think that I've got a reasonable handle on it now.

Some thoughts: due to the "train on contiguous sequences keeping the hidden state" thing, we should not shuffle this dataset.  That
exposes an issue -- how can we get a solid test set?  I'm thinking maybe just hold back the last 20% or something?

Vague discussion with ChatGPT, and that leads to another point: what do we do with batches?  We need some kind of continuous threads going through.

Thought: perhaps we need to be breaking out into batches inside the dataloader?  That's what K seems to be doing.  Need to read and understand the code better.


20205-09-10
Back to it.  Looking at the CharSplitLMMinibatchLoader, it takes the dir to load the data from, the batch size, the sequence length, and the split fractions
(train/test/eval).

It then loads the data into an xs array of tensors, and generates the y's as a shifted copy of the xs

Then it generates its batches!  Considering just xs, we do this:

self.x_batches = data:view(batch_size, -1):split(seq_length, 2)  -- #rows = #batches

The last bit is a comment.

Working through it bit by bit:

data:view(batch_size, -1)

We slice our data into batch_size contiguous sequences.  If we had infinite sequence length, then our batch would be exactly this:
we'd train on all of those sequences in parallel.  Imagine

[t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11]

...then our batched inputs would be this:

[[t0 t1 t2 t3]
 [t4 t5 t6 t7]
 [t8 t9 t10 t11]]

Now we want to reshape again to get sequences.  Imagine our sequence length was 2.  We'd want to train on [t0, t1], backprop, then
keep the hidden state and train on [t2, t3] and then backprop again.  In parallel (with a different hidden state) we'd want to
train on [t4, t5], backprop, [t6, t7].

The split(seq_length, 2) means "split the second dimension into seq_length-long bits".  I'm somewhat struggling to conceptualise that.

Let's look at the result.  It's this:

[
    [
        [t0 t1]
        [t4 t5]
        [ti t9]
    ]
    [
        [t2 t2]
        [t6 t7]
        [t10 t11]
    ]
]

So that does indeed give us two batches such that we can train on it.

Need to work on this to get a better intuition, but I can at least see that it works.

Anyway, this fights against the Dataset abstraction that would normally be used with PyTorch -- it breaks things
up into sequences first.

But!  We know that sequence i is contiguous with i+1.

So, what say we divide our sequences into batch_size sections.  Each of these is a coherent training set.  We can drop the
extras.

We then "stack them" vertically.

Now we have an array like the one we want, and we can split that into train, val, test sets.

OK, let's try doing that but TDD.  I'll backfill tests for NextByteDataset first.








What that means is that first it splits the xs into contiguous slices

But then we split













































