The plan: repro as much as possible of Karpathy's blog post The Unreasonable Effectiveness of Recurrent Neural Networks

2025-09-03

Found article: https://karpathy.github.io/2015/05/21/rnn-effectiveness/

Read through, there's a lot there!

# Recurrent Neural Networks

Diffs between VNNs and CNNs and RNNs: fixed-size input vector.  Interestingly, LLMs have a fixed-size input -- the context length.
Likewise fixed number of computational steps.  This is touted as an advantage, though of course there's no real value in
spending as long completing "the fat cat sat on the mat" as you do completing a 100k token chat transcript.

Do real-world LLMs trim the sequence length to match the actual size?  Using padding tokens feels like a waste.  But then
batches are hard as you'd want them all to be the same size.

But of course for RNNs the hidden state is fixed-size so you have the bottleneck.  It doesn't matter how long you spend
thinking about stuff if you're forgetting things all of the time.

RNNs being turing-complete presumably assumes infinite precision in the hidden state.

Sequences for not-obviously sequential stuff (eg. gaze pattern) is interesting, I remember reading about that
at the time.

His explanation of RNNs is interesting and while compatible with my own, differs
in some interesting ways.  My "hidden states are hidden outputs" is a useful thing for understanding how things are
unrolled, but the practice is of course different.  Perhaps that doesn't matter in actual training?

tanh for activation function interesting, and I note it's only applied to the hidden state update (and the Y is generated
purely on the hidden state after the update, which makes sense otherwise you'd need to do it on X and then H).

Worth noting: to do multi-layer, he just defines a class then calls it twice.  Makes sense!  You can just do that and
then run backwards with PyTorch, I think.  Will be worth experimenting.

LSTMs are used in the essay, and they're more complex.  Worth noting.  Will want to dig into that, I think.


# Character-Level Language Models

OK, so we're not tokenising.  That's important and part of what makes this interesting to me.

Explanation is clear, but the error function is not specified and we'll need to think about that.

Note softmax is cropping up again!

"The RNN is trained with mini-batch Stochastic Gradient Descent and I like to use RMSProp or Adam (per-parameter adaptive learning rate methods) to stablilize the updates."

Code at https://gist.github.com/karpathy/d4dee566867f8291f086 looks useful to learn from.  However, it's RNN rather than LSTM.

https://github.com/karpathy/char-rnn perhaps more relevant?  It links to https://cs.stanford.edu/people/karpathy/char-rnn/, which has the shakespeare, war and peace and wikipedia datasets.  Not the PG one, of course -- copyright would be a problem there.  And the Linux one is just the kernel, not the full dataset he trained on.

# Fun with RNNs

## Paul Graham generator

"this can be cut in half with 50 character BPTT at negligible cost in performance" -- asked ChatGPT and after some discussion it tells me that my gloss on what it said as "with 50-character BPTT, we run a 100-character sequence through but only backprop through the last 50 characters-worth" is a good summary.

"We can also play with the temperature of the Softmax during sampling".  Interesting.  I'd thought of temperature as being
something used when selecting from output options provided by the probability distribution provided by softmax, not an input into
softmax itself.  Chatting with ChatGPT, I came to a better understanding.  My summary (confirmed by it):

We never use argmax -- instead, we choose an output value by dividing our logits by the temperature, then running it through softmax. That gives us a probability distribution, and we always pick from that based on probabilities, so regardless of temperature, if softmax gives us (0.1, 0.4, 0.5) we always pick the first one 10% of the time, the second 40%, and the last 50%.

So, Karpathy is not being technically accurate (though he is phrasing things exactly as people in the industry do).  Temperature
is not part of softmax itself, but nor is it (like I'd imagined) a parameter used when choosing a next character based on the
post-softmax probabilities.

Instead, we just divide the logits by it before running them through softmax.  This has the effect of flattening the distribution
if T > 1, or sharpening it if T < 1.  That also explains why he says "setting temperature very near zero will give the most likely thing that Paul Graham might say" -- if we set it to exactly zero we'd get a divide by zero.

Might be worth a post or at least a callout somewhere later in the LLM from scratch series or in this one.  With a worked example showing
how the distribution changes.


Other examples interesting but not much to learn there.


# Understanding what’s going on

All seems reasonably clear, but there are some nice visualisations.  I particularly like the ones showing the text with the colour
whether the neuron under examination is firing.


# Source code

I think that it would be good to build my own, with LLM help, but his source will be a useful reference as needed.

Interesting reference that Torch was cool then -- but of course PyTorch won out.

# Further Reading

Note lots of references to Sutskever

"Currently it seems that word-level models work better than character-level models, but this is surely a temporary thing" -- hmm

"A second issue is they unnecessarily couple their representation size to the amount of computation per step. For instance, if you double the size of the hidden state vector you’d quadruple the amount of FLOPS at each step due to the matrix multiplication. Ideally, we’d like to maintain a huge representation/memory (e.g. containing all of Wikipedia or many intermediate state variables), while maintaining the ability to keep computation per time step fixed."

Hinting at fixed-length bottleneck here.

"The concept of attention is the most interesting recent architectural innovation in neural networks."

Interesting that attention gets mentioned.  Same meaning as now?  (Obviously not identical, but...)

"unfortunately one sacrifices efficiency because everything that can be attended to is attended to" well yes.

"This then calls for use of techniques from the Reinforcement Learning literature (e.g. REINFORCE) where people are perfectly used to the concept of non-differentiable interactions"

Hmmm.  So RL does not depend on differentiability.  That pushes it high up my list of things to learn.

The "People" bit at the end has what look like interesting links (though they go to individuals' homepages rather than the papers in question)










----ends


Chat with ChatGPT: https://chatgpt.com/c/68b87061-0b58-8326-98d6-1be84411d10e

Some interesting stuff about RNNs (some of which is mixed in above).

I was originally thinking of a post titled "reproducing karpathy" but thinking about it, maybe more of a "revisiting Karpathy".
Or something more specific, it would be silly to imply that this is the most important (or even one of the top ten most important)
things he's done.

Proposed path:

1. intro the article, brief summary, perhaps some high-level gloss on the above.
2. implementation: use PyTorch's built-in LSTM, with parameters chosen from the article/accompanying source.  Train and generate on K's
test data, see if we get similar results.  I think this will need to have the temperature calculations I discussed with ChatGPT.
3. building an LSTM, step 1: build an RNN from "scratch" (in PyTorch, of course, so not reall from scratch).  Try to train, show vanishing gradients problem
4. Step 2, make it an LSTM.  Show it works.


Series title, brainstormed with ChatGPT:

Old-school language models
Vintage language models
Retro language models

This as a start for each post's title, with the following per-post titles:

Revisiting Karpathy’s “The Unreasonable Effectiveness of Recurrent Neural Networks”
Reproducing results with PyTorch’s LSTM
Rolling our own RNN cell
From RNN to LSTM

I'm sure of the first, the next three could do with some tightening




2025-09-04

Let's get started with the Shakespeare example, given that the Paul Graham dataset isn't available (for good reason).

First step, a dummy script to make sure that I understand the imports and so on (I've set up a uv environment with
ipython, numpy and PyTorch):

```
giles@perry:~/Dev/karpathy-repro (main)$ uv run shakespeare.py
giles@perry:~/Dev/karpathy-repro (main)$ cat shakespeare.py
import torch


lstm = torch.nn.LSTM(input_size=2, hidden_size=512, num_layers=3)
giles@perry:~/Dev/karpathy-repro (main)$ cat pyproject.toml
[project]
name = "karpathy-repro"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "ipython>=9.5.0",
    "numpy>=2.3.2",
    "torch>=2.8.0",
]
```


OK, we have working imports -- and the hidden size and num_layers match the blog post.

Now, let's work out what our input size is.  We're doing characters in one-hot vectors, so it will be
something to do with the character vocab.  Simplest possible thing would be to assume ASCII and have 128 inputs, but let's see the source
at https://github.com/karpathy/char-rnn

Note on the code that I spotted almost right away: I think he's doing the backward pass manually!  See lines 227-293 in train.lua

OK, at lines 111-112 we have:

local loader = CharSplitLMMinibatchLoader.create(opt.data_dir, opt.batch_size, opt.seq_length, split_sizes)
local vocab_size = loader.vocab_size  -- the number of distinct characters

...and at lines 147 onwards we create the model (it supports LSTM, GRU -- whatever that is -- and RNN) passing in vocab_size as the first parameter.

So, we're dynamically working out the vocab size from the input.  Shiny!

CharSplitLMMinibatchLoader is a Lua Torch thing.  Let's see what PyTorch has to offer.

https://docs.pytorch.org/docs/stable/data.html looks like a good starting point.  Now, DataLoader has as its first parameter a Dataset object.
That take us to https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset.  Not much help there, though.

Let's work out what his code is actually doing.  CharSplitLMMinibatchLoader is a class that he defines, in https://github.com/karpathy/char-rnn/blob/master/util/CharSplitLMMinibatchLoader.lua

It looks in the given directory for a file called input.txt, and also vocab.t7 and data.t7.  These are the input text, a vocab file and
a tensor file.  If they don't exist, it creates them with CharSplitLMMinibatchLoader.text_to_tensor.

That function loads up the input file, iterates over all of the characters and puts them into a table (~=dict) called unordered where a character
maps to true if it exists.  It also keeps track of characters read in tot_len.

It then iterates over that table, and puts it into another table mapping from a number 1..N to the actual character, and then sorts that -- I assume
by key.

Finally, it swaps it around so that it's mapping character to int ID, calling that vocab_mapping.  That is what will be saved to the vocab file (this happens at the end of the
function).  It uses torch.save for that, but it's just a lua table, not anything torch-specific.

It then creates a tensor version of the input data.  It starts by creating a torch.ByteTensor (so now we're getting some useful info about how
it handles charsets -- it doesn't!  it's actually a byte-level RNN).  The length is tot_len, which is the number of bytes in the file.  Then it opens
the file again and iterates over the characters, and uses the vocab_mapping table to work out which int ID they have, and pops them into the tensor.

Finally, it saves the vocab mapping (as I said before) and the tensor file.

Well, yuk.  I don't know how much of that mess is due to lua being lua (you have to read into a buffer and manage short reads like you would in
C, for example), and how much is academics-can't-code, but I'm sure it would be easier in Python.  Let's give that a go.






















It then, through some
weird and wonderful lua code, allocates an int to each one, so we have a table mapping characters to ints.










